---
title: "Empirical Assignment: Regression Discontinuity"
author: "Michaela Philip"
execute:
  echo: false
jupyter: python3
---

## All data from Keith M. Marzilli Ericson (2014)

```{python}
import pandas as pd 
import matplotlib.pyplot as plt
import warnings
%matplotlib inline
warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)
```


##### 1. Recreate the table of descriptive statistics (Table 1) from Ericson (2014).

```{python}
from analysis.q1 import table_1
table_1
```

\newpage 

##### 2. Recreate Figure 3 from Ericson (2014).

```{python}
plt.close()
from analysis.q2 import scatter_data, lin_data_sort, poly_data_sort
ytitle = "Log Enrollment Share, 2006"
plt.scatter(scatter_data['bin'], scatter_data['bin_scatter'], label='Scatter')
plt.plot(lin_data_sort['lis_premium'], lin_data_sort['local_lin'], linestyle='dashed', color='gray', label='Local Linear')
plt.plot(poly_data_sort['lis_premium'], poly_data_sort['quart_poly'], linestyle='solid', color='black', label='Quartic Polynomial')
plt.xlabel("Monthly Premium - LIS Subsidy, 2006")
plt.ylabel(ytitle)
plt.title("Figure 3")
plt.legend()
plt.show()
```

\newpage

##### 3. Calonico, Cattaneo, and Titiunik (2015) discuss the appropriate partition size for binned scatterplots such as that in Figure 3 of Ericson (2014). More formally, denote by $\mathbb{P}_{-,n} = \{P_{-,j} : j = 1, 2, ...J_{-,n}\}$ and $\mathbb{P_{+,n}} = \{P_{+,j} : j = 1, 2, ...J_{+,n}\}$ the partitions of the support of the running variable $x_i$ on the left and right (respectively) of the cutoff, $\bar{x}$. $P_{-,j}$ and $P_{+,n}$ denote the actual supports for each $j$ partition of size $J_{-,n}$ and $J_{+,n}$ such that $[x_l, \bar{x}) = \cup_{j=1}^{J_{-.n}} P_{-,j}$ and $(\bar{x}, x_u] = \cup_{j=1}^{J_{+,n}} P_{+,j}$. Individual bins are denoted by $p_{-,j}$ and $p_{+,j}$. With this notation in hand, we can write the partitions $J_{-,n}$ and $J_{+,n}$ with equally-spaced bins as $$ p_{-,j} = x_l + j \times \frac{\bar{x}-x_l}{J_{-,n}}$$ and $$p_{+,j} = \bar{x} + j \times \frac{x_u - \bar{x}}{J_{+,n}}$$ Recreate Figure 3 from Ericson (2014) using $J_{-,n} = J_{+,n} = 10$ and $J_{-,n} = J_{+,n} = 30$ Discuss your results and compare them to your figure in Part 2. 

```{python}
plt.close()
from analysis.q3 import scatter_data, lin_data_sort, poly_data_sort
ytitle = "Log Enrollment Share, 2006"
plt.scatter(scatter_data['bin'], scatter_data['bin_scatter'], label='Scatter')
plt.plot(lin_data_sort['lis_premium'], lin_data_sort['local_lin'], linestyle='dashed', color='gray', label='Local Linear')
plt.plot(poly_data_sort['lis_premium'], poly_data_sort['quart_poly'], linestyle='solid', color='black', label='Quartic Polynomial')
plt.xlabel("Monthly Premium - LIS Subsidy, 2006")
plt.ylabel(ytitle)
plt.title("Effect of Benchmark Status on Enrollment (10 Bins)")
plt.legend()
plt.show()
```

Visually, using 10 bins seems to fit the data pretty well. The fit in the middle is very similar to Ericson's Figure 3, and although the two extremes aren't perfect they are not worse than Ericson's figure and may fit a bit better. 

```{python}
plt.close()
from analysis.q3 import scatter_data_2

ytitle = "Log Enrollment Share, 2006"
plt.scatter(scatter_data_2['bin'], scatter_data_2['bin_scatter'], label='Scatter')
plt.plot(lin_data_sort['lis_premium'], lin_data_sort['local_lin'], linestyle='dashed', color='gray', label='Local Linear')
plt.plot(poly_data_sort['lis_premium'], poly_data_sort['quart_poly'], linestyle='solid', color='black', label='Quartic Polynomial')
plt.xlabel("Monthly Premium - LIS Subsidy, 2006")
plt.ylabel(ytitle)
plt.title("Effect of Benchmark Status on Enrollment (30 Bins)")
plt.legend()
plt.show()
```

Using 30 bins for this graph seems to mostly have included a lot of noise - the fit is not significantly better but the graph looks much more busy.

\newpage

##### 4. With the notation above, Calonico, Cattaneo, and Titiunik (2015) derive the optimal number of partitions for an evenly-spaced (ES) RD plot. They show that $$J_{ES, -, n} = [\frac{V_-}{\mathbb{V}_{ES,-}} \frac{n}{\log(n)^2}]$$ and $$J_{ES, +, n} = [\frac{V_+}{\mathbb{V}_{ES,+}} \frac{n}{\log(n)^2}]$$ where $V_-$ and $V_+$ denote the sample variance of the subsamples to the left and right of the cutoff and $\nu_{ES}$ is an integrated variance term derived in the paper. Use the rdrobust package in R (or Stata or Python) to find the optimal number of bins with an evenly-spaced binning strategy. Report this bin count and recreate your binned scatterplots from parts 2 and 3 based on the optimal bin number.

```{python}
plt.close()
from analysis.q4 import bin_scatter
x = bin_scatter.vars_bins.rdplot_mean_x
y= bin_scatter.vars_bins.rdplot_mean_y
n_bins = len(x)
```

```{python}
ytitle = "Log Enrollment Share, 2006"
plt.scatter(x, y, label='Scatter')
plt.plot(lin_data_sort['lis_premium'], lin_data_sort['local_lin'], linestyle='dashed', color='gray', label='Local Linear')
plt.plot(poly_data_sort['lis_premium'], poly_data_sort['quart_poly'], linestyle='solid', color='black', label='Quartic Polynomial')
plt.xlabel("Monthly Premium - LIS Subsidy, 2006")
plt.ylabel(ytitle)
plt.title("Effect of Benchmark Status on Enrollment (11 Bins)")
plt.legend()
plt.show()
print("The optimal number of bins is " + str(n_bins) + ".")
```

\newpage

##### 5. One key underlying assumption for RD design is that agents cannot precisely manipulate the running variable. While “precisely” is not very scientific, we can at least test for whether there appears to be a discrete jump in the running variable around the threshold. Evidence of such a jump may suggest that manipulation is present. Provide the results from the manipulation tests described in Cattaneo, Jansson, and Ma (2018). This test can be implemented with the rddensity package in R, Stata, or Python.

```{python}
plt.close()
from analysis.q5 import rd_test_pval, rd_test_diff, density_plot

print("The p-value for the manipulation test is " + str(rd_test_pval))
print("The difference in estimated density at the cutoff is " + str((rd_test_diff.right - rd_test_diff.left).round(4)))
density_plot
```

\newpage

##### 6. Recreate Panels A and B of Table 3 in Ericson (2014) using the same bandwidth of $4.00 but without any covariates.

```{python}
from analysis.q6 import table_3 as table_3
table_3
```

\newpage

##### 7. Calonico, Cattaneo, and Farrell (2020) show that pre-existing optimal bandwidth calculations (such as those used in Ericson (2014)) are invalid for appropriate inference. They propose an alternative method to derive minimal coverage error (CE)-optimal bandwidths. Re-estimate your RD results using the CE-optimal bandwidth (rdrobust will do this for you) and compare the bandwidth and RD estimates to that in Table 3 of Ericson (2014).

```{python}
from analysis.q7 import table_4
table_4
```

The bandwidth at the beginning of the time frame is very close to Ericson's bandwidth of $4. Over time, however, the optimal bandwidth grows significantly, so choosing one bandwidth for the entire analysis may not be optimal. The signs for all of my results are opposite to Ericson's which was initially alarming, but the interpretation appears to be the same as Ericson's conclusion - pricing lower in 2006 led to increasing market shares in the following years. 

\newpage

##### 8. Now let’s extend the analysis in Section V of Ericson (2014) using IV. Use the presence of Part D low-income subsidy as an IV for market share to examine the effect of market share in 2006 on future premium changes.

```{python}
from analysis.q8 import iv
iv.summary
```

\newpage

##### 9. Discuss your findings and compare results from different binwidths and bandwidths. Compare your results in part 8 to the invest-then-harvest estimates from Table 4 in Ericson (2014).

My dependent variable for part 8 was the difference in premium from 2006 to 2007, so we can interpret these results to say that an increased market share, as instrumented by the presence of a low-income subsidy, led to an increase in premiums from 2006 to 2007. This conclusion is consistent with the 'invest-then-harvest' theory and results from Table 4. 

It doesn't seem as though different binwidths make a large difference in visual results, but using your program to choose the optimal binwidth could ensure that you provide a clear visual that fits the data while reducing noise. 

Choosing different bandwidths do tend to affect the results more and, at least in this case, choosing one bandwidth for several years of data does not seem to be the best option. It seems like for contexts like healthcare, where prices can grow very rapidly, allowing for a bandwidth that changes over time may help you make the clearest comparisons. 

##### 10. Reflect on this assignment. What did you find most challenging? What did you find most surprising?

I found it surprisingly challenging to learn to use the rdplot and rddensity packages. Most packages I have used have very clear and established documentation online, so finding information about different commands or functionality is quite easy. The best way to get information on these packages, however, was directly from the creator's github. I found it a little difficult to learn to navigate the different documentation and find where the most helpful information was, but it was definitely a helpful experience to have to learn to find what I needed. Even having done it once for the first package made it easier to find what I needed for the second package, so I think it was a beneficial experience. Debugging is also always challenging and frustrating, but it is encouraging to find that I am (slowly but surely) becoming faster and more efficient at it. 